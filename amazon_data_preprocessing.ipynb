{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_list = []\n",
    "\n",
    "mis_count = 0\n",
    "for d in parse('./fasion_data/metadata.json.gz'):\n",
    "    try:\n",
    "        if 'Clothing' in d['categories'][0][0]:\n",
    "            line_list.append(d)\n",
    "    except Exception as e:\n",
    "        mis_count += 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435869"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('./fasion_data/fashion_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리\n",
    "\n",
    "### 1단계 : Clothing, Shoes & Jewelry\n",
    "\n",
    "### 2단계 : Women || Men || Girls || Boys\n",
    "\n",
    "### 3단계 : Clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 필터링\n",
    "\n",
    "### 제외 : 3단계 까지의 카테고리\n",
    "\n",
    "### 4단계 카테고리 까지만\n",
    "\n",
    "### [ 4단계 기준 ] 카테고리 44개, 데이터 265742개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cate_dict = {}\n",
    "filter_list = []\n",
    "\n",
    "cate_1 = 'Clothing, Shoes & Jewelry'\n",
    "cate_2_list = ['Women', 'Men', 'Girls', 'Boys']\n",
    "cate_3_list = ['Clothing']\n",
    "\n",
    "for each in df.iterrows():\n",
    "    cate_length = len(each[1]['categories'][0])\n",
    "    \n",
    "    if cate_length < 5:\n",
    "        continue\n",
    "    cate_2 = each[1]['categories'][0][1]\n",
    "    cate_3 = each[1]['categories'][0][2]\n",
    "    cate_4 = each[1]['categories'][0][3]\n",
    "    \n",
    "    if cate_3 in cate_3_list:\n",
    "        filter_list.append(each[1])\n",
    "        cate = cate_1 + '||' + cate_2 + '||' + cate_3 + '||' + cate_4\n",
    "        if cate in cate_dict:    \n",
    "            cate_dict[cate] += 1\n",
    "        else:\n",
    "            cate_dict[cate] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Clothing, Shoes & Jewelry||Boys||Clothing||Active': 436,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Button-Down & Dress Shirts': 586,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Clothing Sets': 1457,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Jackets & Coats': 1469,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Sleepwear & Robes': 1896,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Socks': 225,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Suits & Sport Coats': 677,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Sweaters': 242,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Swim': 976,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Tops & Tees': 3460,\n",
       " 'Clothing, Shoes & Jewelry||Boys||Clothing||Underwear': 547,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Active': 213,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Clothing Sets': 3039,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Dresses': 8773,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Jackets & Coats': 1631,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Sleepwear & Robes': 2372,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Socks & Tights': 998,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Sweaters': 392,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Swim': 1701,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Tops & Tees': 2586,\n",
       " 'Clothing, Shoes & Jewelry||Girls||Clothing||Underwear': 685,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Active': 8217,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Jackets & Coats': 6697,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Pants': 3278,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Shirts': 23857,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Shorts': 2096,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Sleep & Lounge': 2865,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Socks': 2265,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Suits & Sport Coats': 2700,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Sweaters': 2679,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Swim': 3118,\n",
       " 'Clothing, Shoes & Jewelry||Men||Clothing||Underwear': 7150,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Active': 5280,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Coats & Jackets': 8365,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Dresses': 37452,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Lingerie, Sleep & Lounge': 36327,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Pants': 4366,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Shorts': 2795,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Skirts': 5811,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Socks & Hosiery': 6936,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Suits & Blazers': 2348,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Sweaters': 9881,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Swimsuits & Cover Ups': 11773,\n",
       " 'Clothing, Shoes & Jewelry||Women||Clothing||Tops & Tees': 35081}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265742"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_df = pd.DataFrame(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265742"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_df.to_pickle('./fasion_data/filtered_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_df = pd.read_pickle('./fasion_data/filtered_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'float' object has no attribute 'find'\n",
      "'float' object has no attribute 'find'\n",
      "'float' object has no attribute 'find'\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "not_count = 0\n",
    "dir_num = 0\n",
    "for each in filtered_df.iterrows():\n",
    "    each = each[1]\n",
    "    if count % 30000 == 0:\n",
    "        dir_num += 1\n",
    "    f_name = 'dir_' + str(dir_num) + '/' + each['asin']\n",
    "    try:\n",
    "        file_name = wget.download(each['imUrl'], out= f_name)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        not_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if the files are image file \n",
    "### if non verified file then remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imghdr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = (glob.glob('dir_9/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "remove_file_list = []\n",
    "for file_name in file_list:\n",
    "    if (imghdr.what(file_name)) != 'jpeg':\n",
    "        remove_file_list.append(file_name)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dir_9/B00H4UM3YK'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_file_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(remove_file_list)):\n",
    "    os.remove(remove_file_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop category if has less then 1000 data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "filtered_df = joblib.load('./fasion_data/filtered_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "c = 0\n",
    "\n",
    "cate_filter_list = []\n",
    "for each in filtered_df.groupby('cate_1_4'):\n",
    "    if len(each[1]) > 1000:\n",
    "        cate_count = 0\n",
    "        for data in each[1].iterrows():\n",
    "            if cate_count > 2000:\n",
    "                break\n",
    "            cate_filter_list.append(data[1])\n",
    "            cate_count += 1\n",
    "cate_filtered_df = pd.DataFrame(cate_filter_list)\n",
    "cate_filtered_df.to_pickle('./fasion_data/cate_filtered_df_08_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Clothing, Shoes & Jewelry||Boys||Clothing||Clothing Sets', 1458)\n",
      "('Clothing, Shoes & Jewelry||Boys||Clothing||Jackets & Coats', 1470)\n",
      "('Clothing, Shoes & Jewelry||Boys||Clothing||Sleepwear & Robes', 1897)\n",
      "('Clothing, Shoes & Jewelry||Boys||Clothing||Tops & Tees', 2001)\n",
      "('Clothing, Shoes & Jewelry||Girls||Clothing||Clothing Sets', 2001)\n",
      "('Clothing, Shoes & Jewelry||Girls||Clothing||Dresses', 2001)\n",
      "('Clothing, Shoes & Jewelry||Girls||Clothing||Jackets & Coats', 1632)\n",
      "('Clothing, Shoes & Jewelry||Girls||Clothing||Sleepwear & Robes', 2001)\n",
      "('Clothing, Shoes & Jewelry||Girls||Clothing||Swim', 1702)\n",
      "('Clothing, Shoes & Jewelry||Girls||Clothing||Tops & Tees', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Active', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Jackets & Coats', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Pants', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Shirts', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Shorts', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Sleep & Lounge', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Socks', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Suits & Sport Coats', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Sweaters', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Swim', 2001)\n",
      "('Clothing, Shoes & Jewelry||Men||Clothing||Underwear', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Active', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Coats & Jackets', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Dresses', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Lingerie, Sleep & Lounge', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Pants', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Shorts', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Skirts', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Socks & Hosiery', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Suits & Blazers', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Sweaters', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Swimsuits & Cover Ups', 2001)\n",
      "('Clothing, Shoes & Jewelry||Women||Clothing||Tops & Tees', 2001)\n"
     ]
    }
   ],
   "source": [
    "cate_dict = {}\n",
    "c = 0\n",
    "for each in cate_filtered_df.groupby('cate_1_4'):\n",
    "    print(each[0], len(each[1]))\n",
    "    cate_dict[each[0]] = c\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasion_data/filtered_cate_mapping_dict_08_04']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cate_dict, 'fasion_data/filtered_cate_mapping_dict_08_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_list = (glob.glob('amazon_train_img/*'))\n",
    "\n",
    "file_name_list = []\n",
    "class_label_list = []\n",
    "\n",
    "c = 0\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        f_name = file_name.split('/')[1]\n",
    "        class_label = cate_dict[cate_filtered_df[cate_filtered_df['asin'] == f_name]['cate_1_4'].values[0]]\n",
    "        file_name_list.append(file_name)\n",
    "        class_label_list.append(class_label)\n",
    "    except Exception as e:\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./fasion_data/train_class_label_list_08_04']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(file_name_list, './fasion_data/train_file_name_list_08_04')\n",
    "joblib.dump(class_label_list, './fasion_data/train_class_label_list_08_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4LLCdgl.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4X0XeGz.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4f8odig.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4M0XMe2.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4_j2y88.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4Hnwges.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4QntBEf.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4xcbDwp.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4cRTqLX.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4jRylvN.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4cb3Xw6.tmp\n",
      "\n",
      "\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "amazon_train_img/B003LL1XH4cJ__lU.tmp\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name_list = []\n",
    "class_label_list = []\n",
    "\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        f_name = file_name.split('/')[1]\n",
    "        class_label = cate_dict[filtered_df[filtered_df['asin'] == f_name]['cate_1_4'].values[0]]\n",
    "\n",
    "        file_name_list.append(file_name)\n",
    "        class_label_list.append(class_label)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file_name)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./fasion_data/train_class_label_list']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(file_name_list, './fasion_data/train_file_name_list')\n",
    "joblib.dump(class_label_list, './fasion_data/train_class_label_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon_train_img/B008I5834O', 'amazon_train_img/B00804W4A2', 'amazon_train_img/B0081L62XO', 'amazon_train_img/B00B659GYE', 'amazon_train_img/B00E4H80NY', 'amazon_train_img/B009ERED6C', 'amazon_train_img/B00EKPR9HI', 'amazon_train_img/B00DIW9BQQ', 'amazon_train_img/B000PVV4P4', 'amazon_train_img/B009JLWPJA']\n",
      "[16, 3, 34, 1, 31, 5, 23, 43, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "print(file_name_list[0:10])\n",
    "print(class_label_list[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hash table to map file name list with class label list for trining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = tf.contrib.lookup.HashTable(\n",
    "        tf.contrib.lookup.KeyValueTensorInitializer(file_name_list, class_label_list, key_dtype=tf.string, value_dtype=tf.int32), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set image size to 128 and do some processing stuff for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(tf.gfile.Glob(\"amazon_train_img/B0081L62XO\"))\n",
    "image_reader = tf.WholeFileReader()\n",
    "\n",
    "file_path, image_file = image_reader.read(filename_queue)\n",
    "image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "longer = tf.reduce_max(tf.shape(image))\n",
    "image = tf.image.resize_image_with_crop_or_pad(image, longer, longer)\n",
    "image = tf.image.resize_images(image, [128, 128], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (128, 32, 32, 3)\n",
    "### (128, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B008I5834O', 16, 0)\n",
      "(256, 256, 3)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "amazon_data_batch = open('./fasion_data/amazon_data_batch_1.bin', 'wb')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for file_name in file_list:\n",
    "    # In this case f_name == asin \n",
    "    f_name = file_name.split('/')[1]\n",
    "    class_label = cate_dict[filtered_df[filtered_df['asin'] == f_name]['cate_1_4'].values[0]]\n",
    "    class_label_tensor = tf.cast([class_label], tf.int32)\n",
    "    \n",
    "    print(f_name, class_label, count)\n",
    "    \n",
    "\n",
    "    file_name_queue = tf.train.string_input_producer(tf.gfile.Glob(file_name))\n",
    "    image_reader = tf.WholeFileReader()\n",
    "    _, image_file = image_reader.read(file_name_queue)\n",
    "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, 256, 256)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(image.shape)\n",
    "        print(class_label_tensor.shape)\n",
    "        sess.close()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
